{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d8628c",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c230ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450176, 4)\n",
      "   Unnamed: 0                        url   label  result\n",
      "0           0     https://www.google.com  benign       0\n",
      "1           1    https://www.youtube.com  benign       0\n",
      "2           2   https://www.facebook.com  benign       0\n",
      "3           3      https://www.baidu.com  benign       0\n",
      "4           4  https://www.wikipedia.org  benign       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"urldata.csv\")\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8c753bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "0    345738\n",
      "1    104438\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['result'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eed2dd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "url           0\n",
      "label         0\n",
      "result        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5144c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0','result'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eae0ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450133, 2)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['url'].str.startswith('http://') | df['url'].str.startswith('https://')]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3778b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "df_benign = df[df['label'] == 'benign']\n",
    "df_malicious = df[df['label'] == 'malicious']   \n",
    "df_benign_downsampled = resample(df_benign, \n",
    "                                  replace=False,    \n",
    "                                  n_samples=len(df_malicious),     \n",
    "                                  random_state=42)\n",
    "df_balanced = pd.concat([df_benign_downsampled, df_malicious])\n",
    "df_balanced.to_csv(\"balanced_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37d6b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url      label\n",
      "0  https://www.pepworldwide.com.au/industriesaust...     benign\n",
      "1  http://greenbirdeg.net/wp-admin/Mail/AOL/index...  malicious\n",
      "2       http://vonkyngerdy.remotecharity.com/lngopsp  malicious\n",
      "3  https://www.cumberland-mudders.forumotion.com/...     benign\n",
      "4                        http://zytrade.cn/aust7a6ik  malicious\n",
      "5                     http://eribusiness.com/fup9952  malicious\n",
      "6  http://0576tz.com/js/?ref=http://howacewus.bat...  malicious\n",
      "7  https://www.amazon.com/ICO-Shadow-Colossus-Col...     benign\n",
      "8  https://www.wiki.answers.com/Q/What_causes_a_m...     benign\n",
      "9  https://www.wn.com/Football_at_the_1924_Summer...     benign\n"
     ]
    }
   ],
   "source": [
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df_balanced.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0530b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_balanced.tail(200)\n",
    "df_balanced.drop(index=df_test.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae0f3196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 208662/208662 [08:12<00:00, 423.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url      label  \\\n",
      "0  https://www.pepworldwide.com.au/industriesaust...     benign   \n",
      "1  http://greenbirdeg.net/wp-admin/Mail/AOL/index...  malicious   \n",
      "2       http://vonkyngerdy.remotecharity.com/lngopsp  malicious   \n",
      "3  https://www.cumberland-mudders.forumotion.com/...     benign   \n",
      "4                        http://zytrade.cn/aust7a6ik  malicious   \n",
      "5                     http://eribusiness.com/fup9952  malicious   \n",
      "6  http://0576tz.com/js/?ref=http://howacewus.bat...  malicious   \n",
      "7  https://www.amazon.com/ICO-Shadow-Colossus-Col...     benign   \n",
      "8  https://www.wiki.answers.com/Q/What_causes_a_m...     benign   \n",
      "9  https://www.wn.com/Football_at_the_1924_Summer...     benign   \n",
      "\n",
      "   max_url_similarity  \n",
      "0            0.394720  \n",
      "1            0.301664  \n",
      "2            0.430842  \n",
      "3            0.351430  \n",
      "4            0.452831  \n",
      "5            0.506459  \n",
      "6            0.260014  \n",
      "7            0.274900  \n",
      "8            0.330583  \n",
      "9            0.335669  \n"
     ]
    }
   ],
   "source": [
    "from numpy import average\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "def url_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def compute_url_similarities(df, safe_urls):\n",
    "    similarities = []\n",
    "    tqdm.pandas(desc=\"Processing URLs\")\n",
    "    df['max_url_similarity'] = df['url'].progress_apply(\n",
    "        lambda u: average([url_similarity(u, safe_url) for safe_url in safe_urls])\n",
    "    )\n",
    "\n",
    "    return df\n",
    "safe_urls = [\n",
    "    \"https://www.google.com\", \"https://secure.example.com\", \"https://mybank.com\",\n",
    "    \"https://trustedsite.org\", \"https://login.example.com\", \"https://accounts.google.com\",\n",
    "    \"https://github.com\", \"https://openai.com\", \"https://stackoverflow.com\",\n",
    "    \"https://facebook.com\", \"https://linkedin.com\", \"https://microsoft.com\",\n",
    "    \"https://apple.com\", \"https://netflix.com\", \"https://youtube.com\",\n",
    "    \"https://amazon.com\", \"https://flipkart.com\", \"https://snapdeal.com\",\n",
    "    \"https://hdfcbank.com\", \"https://icicibank.com\", \"https://sbi.co.in\",\n",
    "    \"https://canarabank.com\", \"https://paypal.com\", \"https://instamojo.com\",\n",
    "    \"https://razorpay.com\"\n",
    "]\n",
    "df_balanced = compute_url_similarities(df_balanced, safe_urls)\n",
    "print(df_balanced.head(10))\n",
    "df_balanced.to_csv(\"balanced_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7238dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import string\n",
    "import tldextract\n",
    "import ipaddress\n",
    "from urllib.parse import urlparse\n",
    "from googlesearch import search\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "def url_length(url):\n",
    "    return len(url)\n",
    "def domain_length(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        if not isinstance(domain, str):\n",
    "            return 0\n",
    "        return len(domain)\n",
    "    except Exception:\n",
    "        return 0\n",
    "def is_domain_ip(url):\n",
    "    try:\n",
    "        domain = urlparse(url).netloc.split(':')[0]\n",
    "        # Remove whitespace and check for empty or only dots\n",
    "        domain = domain.strip()\n",
    "        if not domain or all(c == '.' for c in domain):\n",
    "            return 0\n",
    "        # Only allow digits and dots for IP check\n",
    "        if not all(c.isdigit() or c == '.' for c in domain):\n",
    "            return 0\n",
    "        ip = ipaddress.ip_address(domain)\n",
    "        if ip.is_private:\n",
    "            return 2  # Private IP\n",
    "        return 1  # Public IP\n",
    "    except Exception:\n",
    "        return 0  # Not an IP\n",
    "def tld_length(url):\n",
    "    return len(tldextract.extract(url).suffix)\n",
    "\n",
    "def no_of_subdomain(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return len(ext.subdomain.split('.')) if ext.subdomain else 0\n",
    "def count_dots(url):\n",
    "    return url.count('.')\n",
    "def count_hyphens_in_domain(url):\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain.count('-')\n",
    "    except:\n",
    "        return 0\n",
    "def count_at_symbols(url):\n",
    "    return url.count('@')\n",
    "def has_port_in_url(url):\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        return 1 if ':' in domain else 0\n",
    "    except:\n",
    "        return 0\n",
    "def is_https(url):\n",
    "    return int(url.lower().startswith('https://'))\n",
    "def is_shortened_url(url):\n",
    "    shortening_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co', 'buff.ly', 'adf.ly', 'is.gd']\n",
    "    try:\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        return 1 if any(service in domain for service in shortening_services) else 0\n",
    "    except:\n",
    "        return 0\n",
    "def has_email_structure(url):\n",
    "    return 1 if re.search(r'\\S+@\\S+', url) else 0\n",
    "def count_slashes(url):\n",
    "    try:\n",
    "        path = urlparse(url).path\n",
    "        return path.count('/')\n",
    "    except:\n",
    "        return 0\n",
    "def count_underscores(url):\n",
    "    return url.count('_')\n",
    "def char_continuation_rate(url):\n",
    "    return sum(url[i] == url[i - 1] for i in range(1, len(url))) / len(url) if url else 0\n",
    "def tld_legitimate_prob(url):\n",
    "    return int(tldextract.extract(url).suffix in {\n",
    "        'com', 'org', 'net', 'edu', 'gov', 'co', 'in', 'uk', 'us', 'de', 'jp'\n",
    "    })\n",
    "def url_char_prob(url):\n",
    "    return sum(c.isalnum() for c in url) / len(url) if url else 0\n",
    "def has_obfuscation(url):\n",
    "    patterns = [r'%[0-9a-fA-F]{2}', r'@', r'\\d{1,3}(\\.\\d{1,3}){3}', r'[-_]{2,}']\n",
    "    return int(any(re.search(p, url) for p in patterns))\n",
    "def no_of_obfuscated_char(url):\n",
    "    return sum(not c.isalnum() for c in url)\n",
    "\n",
    "def obfuscation_ratio(url):\n",
    "    return sum(not c.isalnum() for c in url) / len(url) if url else 0\n",
    "def no_of_letters_in_url(url):\n",
    "    return sum(c.isalpha() for c in url)\n",
    "\n",
    "def letter_ratio_in_url(url):\n",
    "    return sum(c.isalpha() for c in url) / len(url) if url else 0\n",
    "\n",
    "def no_of_digits_in_url(url):\n",
    "    return sum(c.isdigit() for c in url)\n",
    "\n",
    "def digit_ratio_in_url(url):\n",
    "    return sum(c.isdigit() for c in url) / len(url) if url else 0\n",
    "def digit_to_letter_ratio(url):\n",
    "    digits = sum(c.isdigit() for c in url)\n",
    "    letters = sum(c.isalpha() for c in url)\n",
    "    return digits / (letters + 1)\n",
    "def special_char_ratio(url):\n",
    "    special_chars = sum(1 for c in url if c in string.punctuation)\n",
    "    return special_chars / (len(url) + 1)\n",
    "def no_of_equals_in_url(url):\n",
    "    return url.count('=')\n",
    "\n",
    "def no_of_qmark_in_url(url):\n",
    "    return url.count('?')\n",
    "\n",
    "def no_of_ampersand_in_url(url):\n",
    "    return url.count('&')\n",
    "\n",
    "def no_of_other_special_chars_in_url(url):\n",
    "    allowed = set(string.ascii_letters + string.digits + './:?&=-_')\n",
    "    return sum(c not in allowed for c in url)\n",
    "\n",
    "def spacial_char_ratio_in_url(url):\n",
    "    return sum(not c.isalnum() for c in url) / len(url) if url else 0\n",
    "def has_multiple_tlds(url):\n",
    "    tlds = ['.com', '.net', '.org', '.co', '.in', '.ru', '.cn', '.info', '.biz', '.uk', '.us', '.tv', '.cc']\n",
    "    return sum(url.lower().count(tld) for tld in tlds) > 1\n",
    "def has_suspicious_words(url):\n",
    "    suspicious_keywords = ['login', 'verify', 'update', 'free', 'bank', 'secure', 'account', 'password']\n",
    "    url_lower = url.lower()\n",
    "    return 1 if any(word in url_lower for word in suspicious_keywords) else 0\n",
    "def has_suspicious_tld(url):\n",
    "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top', '.club', '.info']\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        for tld in suspicious_tlds:\n",
    "            if domain.endswith(tld):\n",
    "                return 1\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "def calculate_entropy(s):\n",
    "    p, lns = Counter(s), float(len(s))\n",
    "    return -sum(count/lns * log2(count/lns) for count in p.values())\n",
    "def google_index(url):\n",
    "\n",
    "    site = search(url, 5)\n",
    "    return 1 if site else 0\n",
    "def abnormal_url(url):\n",
    "    hostname = urlparse(url).hostname\n",
    "    hostname = str(hostname)\n",
    "    match = re.search(hostname, url)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2290aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_functions = {\n",
    "    'url_length': url_length,\n",
    "    'domain_length': domain_length,\n",
    "    'is_domain_ip': is_domain_ip,\n",
    "    'tld_length': tld_length,\n",
    "    'no_of_subdomain': no_of_subdomain,\n",
    "    'count_dots': count_dots,\n",
    "    'count_hyphens_in_domain': count_hyphens_in_domain,\n",
    "    'count_at_symbols': count_at_symbols,\n",
    "    'has_port_in_url': has_port_in_url,\n",
    "    'is_https': is_https,\n",
    "    'is_shortened_url': is_shortened_url,\n",
    "    'has_email_structure': has_email_structure,\n",
    "    'count_slashes': count_slashes,\n",
    "    'count_underscores': count_underscores,\n",
    "    'char_continuation_rate': char_continuation_rate,\n",
    "    'tld_legitimate_prob': tld_legitimate_prob,\n",
    "    'url_char_prob': url_char_prob,\n",
    "    'has_obfuscation': has_obfuscation,\n",
    "    'no_of_obfuscated_char': no_of_obfuscated_char,\n",
    "    'obfuscation_ratio': obfuscation_ratio,\n",
    "    'no_of_letters_in_url': no_of_letters_in_url,\n",
    "    'letter_ratio_in_url': letter_ratio_in_url,\n",
    "    'no_of_digits_in_url': no_of_digits_in_url,\n",
    "    'digit_ratio_in_url': digit_ratio_in_url,\n",
    "    'digit_to_letter_ratio': digit_to_letter_ratio,\n",
    "    'special_char_ratio': special_char_ratio,\n",
    "    'no_of_equals_in_url': no_of_equals_in_url,\n",
    "    'no_of_qmark_in_url': no_of_qmark_in_url,\n",
    "    'no_of_ampersand_in_url': no_of_ampersand_in_url,\n",
    "    'no_of_other_special_chars_in_url': no_of_other_special_chars_in_url,\n",
    "    'spacial_char_ratio_in_url': spacial_char_ratio_in_url,\n",
    "    'has_multiple_tlds': has_multiple_tlds,\n",
    "    'has_suspicious_words': has_suspicious_words,\n",
    "    'has_suspicious_tld': has_suspicious_tld,\n",
    "    'calculate_entropy': calculate_entropy,\n",
    "    'google_index': google_index,\n",
    "    # 'abnormal_url': abnormal_url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f2c6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 636172.36it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 94076.72it/s] \n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 73993.30it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 104122.55it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 112678.48it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 784430.86it/s] \n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 98938.86it/s] \n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 688662.20it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 94161.63it/s] \n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 603072.77it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 76349.01it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:04<00:00, 43559.95it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 130268.42it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 1038129.38it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:07<00:00, 26816.75it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 81177.84it/s] \n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 144099.63it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 135967.21it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 139567.47it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 138792.78it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 152640.51it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 156357.39it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 156243.92it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 152723.73it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 77071.65it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 187823.96it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 587798.41it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 828431.85it/s] \n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 858941.55it/s] \n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 128090.98it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 140106.13it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 173746.50it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 364206.95it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:01<00:00, 109097.38it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:02<00:00, 72116.83it/s]\n",
      "Extracting features: 100%|██████████| 208662/208662 [00:00<00:00, 743344.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"balanced_data.csv\")\n",
    "tqdm.pandas(desc=\"Extracting features\")\n",
    "features_df = pd.DataFrame({\n",
    "    fname: df['url'].progress_apply(f) for fname, f in selected_feature_functions.items()\n",
    "})\n",
    "features_df['url'] = df['url']\n",
    "balanced_df = pd.read_csv(\"balanced_data.csv\")\n",
    "combined_df = pd.merge(features_df, balanced_df, on='url', how='inner')\n",
    "combined_df.to_csv(\"combined_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "197cd278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208662, 39)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"combined_features.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9931948338245513\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      1.00      0.99     20869\n",
      "   malicious       1.00      0.99      0.99     20864\n",
      "\n",
      "    accuracy                           0.99     41733\n",
      "   macro avg       0.99      0.99      0.99     41733\n",
      "weighted avg       0.99      0.99      0.99     41733\n",
      "\n",
      "Confusion Matrix:\n",
      " [[20820    49]\n",
      " [  235 20629]]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['url', 'label'])\n",
    "y = df['label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29cfbeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url_classifier.pkl']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf, 'url_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2644850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.995\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      1.00      0.99        88\n",
      "   malicious       1.00      0.99      1.00       112\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.99      1.00      0.99       200\n",
      "weighted avg       1.00      0.99      1.00       200\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[ 88   0]\n",
      " [  1 111]]\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import model\n",
    "import joblib\n",
    "test_features_df = pd.DataFrame({\n",
    "    fname: df_test['url'].apply(f) for fname, f in selected_feature_functions.items()\n",
    "})\n",
    "model = joblib.load('url_classifier.pkl')\n",
    "test_features_df['url'] = df_test['url']\n",
    "test_features_df['label'] = df_test['label']\n",
    "from numpy import average\n",
    "\n",
    "test_features_df['max_url_similarity'] = df_test['url'].apply(\n",
    "    lambda u: average([url_similarity(u, safe_url) for safe_url in safe_urls])\n",
    ")\n",
    "\n",
    "X_test = test_features_df.drop(columns=['url', 'label'], errors='ignore')\n",
    "y_test = test_features_df['label']\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60918593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for the URL: benign\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from numpy import average\n",
    "\n",
    "# Sample URL to test\n",
    "df_test = pd.DataFrame(columns=['url'])\n",
    "df_test['url'] = [\"https://www.espncricinfo.com\"]\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load('url_classifier.pkl')\n",
    "\n",
    "# Assuming selected_feature_functions is a dict of {feature_name: function}\n",
    "# Apply feature extraction functions on the input URL\n",
    "test_features = pd.DataFrame({\n",
    "    fname: [f(df_test['url'][0])] for fname, f in selected_feature_functions.items()\n",
    "})\n",
    "safe_urls = [\n",
    "    \"https://www.google.com\", \"https://secure.example.com\", \"https://mybank.com\",\n",
    "    \"https://trustedsite.org\", \"https://login.example.com\", \"https://accounts.google.com\",\n",
    "    \"https://github.com\", \"https://openai.com\", \"https://stackoverflow.com\",\n",
    "    \"https://facebook.com\", \"https://linkedin.com\", \"https://microsoft.com\",\n",
    "    \"https://apple.com\", \"https://netflix.com\", \"https://youtube.com\",\n",
    "    \"https://amazon.com\", \"https://flipkart.com\", \"https://snapdeal.com\",\n",
    "    \"https://hdfcbank.com\", \"https://icicibank.com\", \"https://sbi.co.in\",\n",
    "    \"https://canarabank.com\", \"https://paypal.com\", \"https://instamojo.com\",\n",
    "    \"https://razorpay.com\"\n",
    "]\n",
    "from difflib import SequenceMatcher\n",
    "def url_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "test_features['max_url_similarity'] = average([\n",
    "    url_similarity(df_test['url'][0], safe_url) for safe_url in safe_urls\n",
    "])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(test_features)\n",
    "print(\"Predicted label for the URL:\", y_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ad568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153ae92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
